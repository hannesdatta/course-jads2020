{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! Welcome to the course page for Marketing Analytics , which will be held online to students of the DEP program at JADS 's-Hertogenbosch in May and June 2020. In these 4 sessions, you will learn about cutting-edge techniques for collecting online data, structuring data workflows, and predicting customer behavior. Updated module 4 We have updated the course material for module 4, which now includes the web scraping tutorial, and a corrected link to the setup instructions for chromedriver . Save the dates Session 1: Customer Response Models (George Knox) Date: 29 May 2020, 09.00 - 12.30 Session 2: Efficient workflows for data- and computation-intensive projects (Hannes Datta) Date: 29 May 2020, 13.30 - 17.00 Session 3: Customer Lifetime Value (George Knox) Date: 5 June 2020, 09.00 - 12.30 Session 4: Managing large-scale online data collections using web scraping/APIs (Hannes Datta) Date: 26 June 2020, 09.00 - 12.30 Course material We make use of a shared folder on Dropbox to add files (e.g., our slide decks) that you can use during the sessions. Please also view our syllabus for each session, which you can access via the menu of this site. Computer setup done? If not, ask for help, please! To follow modules 2 and 4 (starting May 29), you need to have set up your computer correctly. If you have problems, please be in touch ASAP - at the latest by Thursday (May 28), so that we can schedule a quick meeting. Please install TeamViewer Please schedule an appointment with us via WhatsApp (+31134668938). Please send us your TeamViewer ID and temporary password via WhatsApp at the agreed time. What's next Check out syllabus and preparation on the left sidebar (desktop) or menu (mobile) to see how to prepare for each session Get to know your instructors (\u2192 about the instructors )","title":"Home"},{"location":"#welcome","text":"Welcome to the course page for Marketing Analytics , which will be held online to students of the DEP program at JADS 's-Hertogenbosch in May and June 2020. In these 4 sessions, you will learn about cutting-edge techniques for collecting online data, structuring data workflows, and predicting customer behavior. Updated module 4 We have updated the course material for module 4, which now includes the web scraping tutorial, and a corrected link to the setup instructions for chromedriver . Save the dates Session 1: Customer Response Models (George Knox) Date: 29 May 2020, 09.00 - 12.30 Session 2: Efficient workflows for data- and computation-intensive projects (Hannes Datta) Date: 29 May 2020, 13.30 - 17.00 Session 3: Customer Lifetime Value (George Knox) Date: 5 June 2020, 09.00 - 12.30 Session 4: Managing large-scale online data collections using web scraping/APIs (Hannes Datta) Date: 26 June 2020, 09.00 - 12.30 Course material We make use of a shared folder on Dropbox to add files (e.g., our slide decks) that you can use during the sessions. Please also view our syllabus for each session, which you can access via the menu of this site. Computer setup done? If not, ask for help, please! To follow modules 2 and 4 (starting May 29), you need to have set up your computer correctly. If you have problems, please be in touch ASAP - at the latest by Thursday (May 28), so that we can schedule a quick meeting. Please install TeamViewer Please schedule an appointment with us via WhatsApp (+31134668938). Please send us your TeamViewer ID and temporary password via WhatsApp at the agreed time.","title":"Welcome!"},{"location":"#whats-next","text":"Check out syllabus and preparation on the left sidebar (desktop) or menu (mobile) to see how to prepare for each session Get to know your instructors (\u2192 about the instructors )","title":"What's next"},{"location":"about/","text":"About the instructors Hannes Datta Hi, I'm Hannes Datta , an Associate Professor of Marketing at Tilburg University. I develop advanced econometric models that guide managerial decision-making and inform public policy in the area of digital media consumption (e.g., on streaming services, on digital TV), branding, and retailing. Next to my research , I have a passion for teaching, and have won several teaching awards in the past years (among others, Tilburg University's Teacher of the Year Award). For my research, I make use of web scraping and API technology, and that's also how the inaugural (2019) version of this course was born. Recently, I have launched an initiative to enhance research productivity by designing efficient workflows to manage data- and computation-intensive projects. That's also why I use this new site and brand-new tutorials throughout this course. The site is open-source, by the way, so feel free to contribute to it! You can best reach me (for short questions) via WhatsApp, on +31134668938. More formal inquiries should be emailed . Links: Website Research LinkedIn Twitter E-mail George Knox Hi, I'm George Knox, an associate professor of marketing at Tilburg University My interests are in developing models to predict and understand customer purchase and shopping behavior across different industries. I have published papers on renting vs. buying, unplanned purchases, customer complaints, digital media consumption, and customer lifetime value. At Tilburg I teach Customer Analytics (masters) and Marketing Analytics for Big Data (bachelors). My site below has more information. Links: Website LinkedIn Twitter E-mail","title":"About the instructors"},{"location":"about/#about-the-instructors","text":"","title":"About the instructors"},{"location":"about/#hannes-datta","text":"Hi, I'm Hannes Datta , an Associate Professor of Marketing at Tilburg University. I develop advanced econometric models that guide managerial decision-making and inform public policy in the area of digital media consumption (e.g., on streaming services, on digital TV), branding, and retailing. Next to my research , I have a passion for teaching, and have won several teaching awards in the past years (among others, Tilburg University's Teacher of the Year Award). For my research, I make use of web scraping and API technology, and that's also how the inaugural (2019) version of this course was born. Recently, I have launched an initiative to enhance research productivity by designing efficient workflows to manage data- and computation-intensive projects. That's also why I use this new site and brand-new tutorials throughout this course. The site is open-source, by the way, so feel free to contribute to it! You can best reach me (for short questions) via WhatsApp, on +31134668938. More formal inquiries should be emailed . Links: Website Research LinkedIn Twitter E-mail","title":"Hannes Datta"},{"location":"about/#george-knox","text":"Hi, I'm George Knox, an associate professor of marketing at Tilburg University My interests are in developing models to predict and understand customer purchase and shopping behavior across different industries. I have published papers on renting vs. buying, unplanned purchases, customer complaints, digital media consumption, and customer lifetime value. At Tilburg I teach Customer Analytics (masters) and Marketing Analytics for Big Data (bachelors). My site below has more information. Links: Website LinkedIn Twitter E-mail","title":"George Knox"},{"location":"schedule/","text":"Schedule The course will be taught on two days. The schedule will be updated once we approach the course. Session 1: Efficient workflows for data- and computation-intensive projects Date: 29 May 2020, 13.30 - 17.00 13.30 - 14.30 tba 14.45 - 15:45 tba 16.00 - 17:00 tba Session 2: \"Web scraping/APIs\" Date: 26 June 2020, 09.00 - 12.30 09.00 - 10.00 tba 10.15 - 11:15 tba 11:30 - 12:30 tba","title":"Schedule"},{"location":"schedule/#schedule","text":"The course will be taught on two days. The schedule will be updated once we approach the course. Session 1: Efficient workflows for data- and computation-intensive projects Date: 29 May 2020, 13.30 - 17.00 13.30 - 14.30 tba 14.45 - 15:45 tba 16.00 - 17:00 tba Session 2: \"Web scraping/APIs\" Date: 26 June 2020, 09.00 - 12.30 09.00 - 10.00 tba 10.15 - 11:15 tba 11:30 - 12:30 tba","title":"Schedule"},{"location":"syllabus/","text":"Course syllabus Session 1: Efficient Workflows for Data- and Computation-intensive Projects Learning Objectives Review the key building blocks of efficient workflows, following tilburgsciencehub.com e.g., understand the concept of project pipelines and project components, and how they apply to your own research project e.g., learn about the benefits of automating your project's pipeline Replicate a reproducible workflow for enriching JSON data from Twitter with text mining metrics, and producing an RMarkdown document with results. Discuss and implement advanced workflows to manage complex computational projects Adhere to a grow-as-you go directory structure to keep source code organized Learn how to automate your project infrastructure using make Integrate cloud storage from AWS S3, Google Drive, or Dropbox Version your project's source code and manage Issues/To do's using Git/GitHub Collaborate on open source projects Formulate steps to professionalize data and code management in your own research projects Preparation To be able to follow the class, you need to prepare the following things: Setup your computer, following our installation guide . Please install: Python 3.x via Anaconda R >= 3.x and RStudio GNU Make It is important that these software tools are callable from the command line/terminal. Mac users: try whether running python , R and make from the terminal works. Windows users: please configure your environment variables so that you can call python , R and make from Anaconda Prompt . Familiarize yourself with http://tilburgsciencehub.com . In particular, browse our section where we document the basics of efficient workflow design explore the various other sections of the website Follow our tutorial to implement a basic automated workflow to enrich JSON data with text mining metrics. Please allow sufficient time to follow this tutorial (approx. 4 hours should suffice, if you have the software setup right). Be prepared to show the directory of a recent project you\u2019re working on \u2013 be able to explain that structure (you don\u2019t need to revise the structure before class!) Session 2: Web scraping and APIs Learning Objectives Overview about the domain of online data collections Introduce you to web scraping and APIs: what are they, exactly, and what are commonalities and differences? Understand the basics of web technology Explain the difference between structured (e.g., Excel files, SQL databases) and unstructured data bases (e.g., MongoDB, JSON objects) Identify relevant (open) databases, websites and APIs that can be useful for your own research projects Explain the data context of your own research Understand data retrieval strategies: forward looking versus backward looking Decide on what information to retrieve: identifying CSS selectors and API endpoints Seeding strategies in online data collections Legal aspects of online data collections Enable you to make the first steps in scraping a website of your choice Preparation Warning Likely to be updated soon. Watch an introduction video to web scraping and APIs (~20 minutes): what are they, what can they be used for? Think about which online data may be useful for your own research; be able to show the website, database or API to everyone in class. Set up your own laptop for in-class exercises On top of the software stack for Session #1, you need to install the \"web scraping tools\"; details are available on https://tilburgsciencehub.com/setup/webscraping Browse my article on how Spotify has changed music consumption . Pay close attention to the data section. Go through the web scraping tutorial (will be made available later).","title":"Course syllabus"},{"location":"syllabus/#course-syllabus","text":"","title":"Course syllabus"},{"location":"syllabus/#session-1-efficient-workflows-for-data-and-computation-intensive-projects","text":"","title":"Session 1: Efficient Workflows for Data- and Computation-intensive Projects"},{"location":"syllabus/#learning-objectives","text":"Review the key building blocks of efficient workflows, following tilburgsciencehub.com e.g., understand the concept of project pipelines and project components, and how they apply to your own research project e.g., learn about the benefits of automating your project's pipeline Replicate a reproducible workflow for enriching JSON data from Twitter with text mining metrics, and producing an RMarkdown document with results. Discuss and implement advanced workflows to manage complex computational projects Adhere to a grow-as-you go directory structure to keep source code organized Learn how to automate your project infrastructure using make Integrate cloud storage from AWS S3, Google Drive, or Dropbox Version your project's source code and manage Issues/To do's using Git/GitHub Collaborate on open source projects Formulate steps to professionalize data and code management in your own research projects","title":"Learning Objectives"},{"location":"syllabus/#preparation","text":"To be able to follow the class, you need to prepare the following things: Setup your computer, following our installation guide . Please install: Python 3.x via Anaconda R >= 3.x and RStudio GNU Make It is important that these software tools are callable from the command line/terminal. Mac users: try whether running python , R and make from the terminal works. Windows users: please configure your environment variables so that you can call python , R and make from Anaconda Prompt . Familiarize yourself with http://tilburgsciencehub.com . In particular, browse our section where we document the basics of efficient workflow design explore the various other sections of the website Follow our tutorial to implement a basic automated workflow to enrich JSON data with text mining metrics. Please allow sufficient time to follow this tutorial (approx. 4 hours should suffice, if you have the software setup right). Be prepared to show the directory of a recent project you\u2019re working on \u2013 be able to explain that structure (you don\u2019t need to revise the structure before class!)","title":"Preparation"},{"location":"syllabus/#session-2-web-scraping-and-apis","text":"","title":"Session 2: Web scraping and APIs"},{"location":"syllabus/#learning-objectives_1","text":"Overview about the domain of online data collections Introduce you to web scraping and APIs: what are they, exactly, and what are commonalities and differences? Understand the basics of web technology Explain the difference between structured (e.g., Excel files, SQL databases) and unstructured data bases (e.g., MongoDB, JSON objects) Identify relevant (open) databases, websites and APIs that can be useful for your own research projects Explain the data context of your own research Understand data retrieval strategies: forward looking versus backward looking Decide on what information to retrieve: identifying CSS selectors and API endpoints Seeding strategies in online data collections Legal aspects of online data collections Enable you to make the first steps in scraping a website of your choice","title":"Learning Objectives"},{"location":"syllabus/#preparation_1","text":"Warning Likely to be updated soon. Watch an introduction video to web scraping and APIs (~20 minutes): what are they, what can they be used for? Think about which online data may be useful for your own research; be able to show the website, database or API to everyone in class. Set up your own laptop for in-class exercises On top of the software stack for Session #1, you need to install the \"web scraping tools\"; details are available on https://tilburgsciencehub.com/setup/webscraping Browse my article on how Spotify has changed music consumption . Pay close attention to the data section. Go through the web scraping tutorial (will be made available later).","title":"Preparation"},{"location":"sessions/clv/","text":"Course syllabus Customer Lifetime Value (George Knox) Learning Objectives Understand how to calculate customer lifetime value in different settings: when customer churn/dropout is observed, e.g., subscription models when it is unobserved, e.g., transactional relationships e.g., grocery shopping Understanding how differences across customers, i.e., heterogeneity, affects retention and lifetime value. Preparation These readings are not required. I will teach you what you need to know in the session. They are meant more as background in case you're interested. Read up on How to Project Customer Retention RFM and CLV Setup your computer, following our installation guide . Please install: R >= 3.x and RStudio We will also use Microsoft Excel","title":"Session 3 (Customer lifetime value)"},{"location":"sessions/clv/#course-syllabus","text":"","title":"Course syllabus"},{"location":"sessions/clv/#customer-lifetime-value-george-knox","text":"","title":"Customer Lifetime Value (George Knox)"},{"location":"sessions/clv/#learning-objectives","text":"Understand how to calculate customer lifetime value in different settings: when customer churn/dropout is observed, e.g., subscription models when it is unobserved, e.g., transactional relationships e.g., grocery shopping Understanding how differences across customers, i.e., heterogeneity, affects retention and lifetime value.","title":"Learning Objectives"},{"location":"sessions/clv/#preparation","text":"These readings are not required. I will teach you what you need to know in the session. They are meant more as background in case you're interested. Read up on How to Project Customer Retention RFM and CLV Setup your computer, following our installation guide . Please install: R >= 3.x and RStudio We will also use Microsoft Excel","title":"Preparation"},{"location":"sessions/customerresponse/","text":"Course syllabus Customer Response Models (George Knox) Learning Objectives Understand, fit and interpret several commonly used customer targeting models Recency-Frequency-Monetary analysis logistic regression decision trees random forests (time permitting) Use them to make better managerial decisions in several areas: direct marketing targeting retention/churn prevention Understand lift and ROC curves Understand how to validate predictive models and avoid overfitting Preparation These readings are not required. I will teach you what you need to know in the session. They are meant more as background in case you're interested. Read up on RFM analysis logistic regression , requires library access decision trees , requires library access Setup your computer, following our installation guide . Please install: R >= 3.x and RStudio We will also use Microsoft Excel","title":"Session 1 (Customer response models)"},{"location":"sessions/customerresponse/#course-syllabus","text":"","title":"Course syllabus"},{"location":"sessions/customerresponse/#customer-response-models-george-knox","text":"","title":"Customer Response Models (George Knox)"},{"location":"sessions/customerresponse/#learning-objectives","text":"Understand, fit and interpret several commonly used customer targeting models Recency-Frequency-Monetary analysis logistic regression decision trees random forests (time permitting) Use them to make better managerial decisions in several areas: direct marketing targeting retention/churn prevention Understand lift and ROC curves Understand how to validate predictive models and avoid overfitting","title":"Learning Objectives"},{"location":"sessions/customerresponse/#preparation","text":"These readings are not required. I will teach you what you need to know in the session. They are meant more as background in case you're interested. Read up on RFM analysis logistic regression , requires library access decision trees , requires library access Setup your computer, following our installation guide . Please install: R >= 3.x and RStudio We will also use Microsoft Excel","title":"Preparation"},{"location":"sessions/scraping/","text":"Course syllabus Managing large-scale online data collections using web scraping and APIs (Hannes Datta) Learning Objectives Overview about the domain of online data collections Introduce you to web scraping and APIs: what are they, exactly, and what are commonalities and differences? Understand the basics of web technology Explain the difference between structured (e.g., Excel files, SQL databases) and unstructured data bases (e.g., MongoDB, JSON objects) Identify relevant (open) databases, websites and APIs that can be useful for your own research projects Explain the data context of your own research Understand data retrieval strategies: forward looking versus backward looking Decide on what information to retrieve: identifying CSS selectors and API endpoints Seeding strategies in online data collections Legal aspects of online data collections Enable you to make the first steps in scraping a website of your choice Preparation Watch an introduction video to web scraping and APIs (~20 minutes): what are they, what can they be used for? Think about which online data may be useful for your own research; be able to show the website, database or API to everyone in class. Set up your own laptop for in-class exercises On top of the software stack for your session on workflows , you need to install the \"web scraping tools\" Details are available on http://tilburgsciencehub.com/setup/webscraping_drivers/ . Please verify you have properly installed these tools by typing chromedriver on your terminal/Anaconda prompt. Browse my article on how Spotify has changed music consumption . Pay close attention to the data section. Go through the web scraping tutorial, which is a Jupyter Notebook ( preview as HTML , download as .ipynb ) that illustrates the basic steps of web scraping. Course material during the workshop workshop.ipynb - exercises during the workshop ( preview as HTML , download as .ipynb ) scraping template ( preview as HTML , download as .ipynb ) API template ( preview as HTML , download as .ipynb )","title":"Session 4 (Web scraping and APIs)"},{"location":"sessions/scraping/#course-syllabus","text":"","title":"Course syllabus"},{"location":"sessions/scraping/#managing-large-scale-online-data-collections-using-web-scraping-and-apis-hannes-datta","text":"","title":"Managing large-scale online data collections using web scraping and APIs (Hannes Datta)"},{"location":"sessions/scraping/#learning-objectives","text":"Overview about the domain of online data collections Introduce you to web scraping and APIs: what are they, exactly, and what are commonalities and differences? Understand the basics of web technology Explain the difference between structured (e.g., Excel files, SQL databases) and unstructured data bases (e.g., MongoDB, JSON objects) Identify relevant (open) databases, websites and APIs that can be useful for your own research projects Explain the data context of your own research Understand data retrieval strategies: forward looking versus backward looking Decide on what information to retrieve: identifying CSS selectors and API endpoints Seeding strategies in online data collections Legal aspects of online data collections Enable you to make the first steps in scraping a website of your choice","title":"Learning Objectives"},{"location":"sessions/scraping/#preparation","text":"Watch an introduction video to web scraping and APIs (~20 minutes): what are they, what can they be used for? Think about which online data may be useful for your own research; be able to show the website, database or API to everyone in class. Set up your own laptop for in-class exercises On top of the software stack for your session on workflows , you need to install the \"web scraping tools\" Details are available on http://tilburgsciencehub.com/setup/webscraping_drivers/ . Please verify you have properly installed these tools by typing chromedriver on your terminal/Anaconda prompt. Browse my article on how Spotify has changed music consumption . Pay close attention to the data section. Go through the web scraping tutorial, which is a Jupyter Notebook ( preview as HTML , download as .ipynb ) that illustrates the basic steps of web scraping.","title":"Preparation"},{"location":"sessions/scraping/#course-material-during-the-workshop","text":"workshop.ipynb - exercises during the workshop ( preview as HTML , download as .ipynb ) scraping template ( preview as HTML , download as .ipynb ) API template ( preview as HTML , download as .ipynb )","title":"Course material during the workshop"},{"location":"sessions/workflows/","text":"Course syllabus Efficient Workflows for Data- and Computation-intensive Projects (Hannes Datta) Learning Objectives Review the key building blocks of efficient workflows, following tilburgsciencehub.com e.g., understand the concept of project pipelines and project components, and how they apply to your own research project e.g., learn about the benefits of automating your project's pipeline Replicate a reproducible workflow for enriching JSON data from Twitter with text mining metrics, and producing an RMarkdown document with results. Discuss and implement advanced workflows to manage complex computational projects Adhere to a grow-as-you go directory structure to keep source code organized Learn how to automate your project infrastructure using make Integrate cloud storage from AWS S3, Google Drive, or Dropbox Version your project's source code and manage Issues/To do's using Git/GitHub Collaborate on open source projects Formulate steps to professionalize data and code management in your own research projects Preparation To be able to follow the class, you need to prepare the following things: Setup your computer, following our installation guide . Please install: Python 3.x via Anaconda . It is not enough to merely have access to a cloud-based version of Jupyter Notebook/Labs via your browser. We actually require a local Anaconda installation. R >= 3.x and RStudio Make (it is preinstalled on Mac) It is important that these software tools are callable from the command line/terminal. Mac users: try whether running python , R and make from the terminal works. Windows users: please configure your environment variables so that you can call python , R and make from Anaconda Prompt . Familiarize yourself with http://tilburgsciencehub.com . In particular, browse our section where we document the basics of efficient workflow design explore the various other sections of the website Follow our tutorial to implement a basic automated workflow to enrich JSON data with text mining metrics. Please allow sufficient time to follow this tutorial (approx. 4 hours should suffice, if you have the software setup right). Be prepared to show the directory of a recent project you\u2019re working on \u2013 be able to explain that structure (you don\u2019t need to revise the structure before class!)","title":"Session 2 (Efficient workflows)"},{"location":"sessions/workflows/#course-syllabus","text":"","title":"Course syllabus"},{"location":"sessions/workflows/#efficient-workflows-for-data-and-computation-intensive-projects-hannes-datta","text":"","title":"Efficient Workflows for Data- and Computation-intensive Projects (Hannes Datta)"},{"location":"sessions/workflows/#learning-objectives","text":"Review the key building blocks of efficient workflows, following tilburgsciencehub.com e.g., understand the concept of project pipelines and project components, and how they apply to your own research project e.g., learn about the benefits of automating your project's pipeline Replicate a reproducible workflow for enriching JSON data from Twitter with text mining metrics, and producing an RMarkdown document with results. Discuss and implement advanced workflows to manage complex computational projects Adhere to a grow-as-you go directory structure to keep source code organized Learn how to automate your project infrastructure using make Integrate cloud storage from AWS S3, Google Drive, or Dropbox Version your project's source code and manage Issues/To do's using Git/GitHub Collaborate on open source projects Formulate steps to professionalize data and code management in your own research projects","title":"Learning Objectives"},{"location":"sessions/workflows/#preparation","text":"To be able to follow the class, you need to prepare the following things: Setup your computer, following our installation guide . Please install: Python 3.x via Anaconda . It is not enough to merely have access to a cloud-based version of Jupyter Notebook/Labs via your browser. We actually require a local Anaconda installation. R >= 3.x and RStudio Make (it is preinstalled on Mac) It is important that these software tools are callable from the command line/terminal. Mac users: try whether running python , R and make from the terminal works. Windows users: please configure your environment variables so that you can call python , R and make from Anaconda Prompt . Familiarize yourself with http://tilburgsciencehub.com . In particular, browse our section where we document the basics of efficient workflow design explore the various other sections of the website Follow our tutorial to implement a basic automated workflow to enrich JSON data with text mining metrics. Please allow sufficient time to follow this tutorial (approx. 4 hours should suffice, if you have the software setup right). Be prepared to show the directory of a recent project you\u2019re working on \u2013 be able to explain that structure (you don\u2019t need to revise the structure before class!)","title":"Preparation"}]}